# ---
# codex-agent:
#   name: Agent.CI
#   role: Runs tests, security scans, and documentation checks for pushes and PRs
#   scope: .github/workflows/ci.yml
#   triggers: Push and pull_request events
#   output: Build artifacts, coverage reports, diagnostics
# ---

name: CI

on:
  push:
  pull_request:

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: write        # push coverage badge
  issues: write          # create/update CI failure issue
  pull-requests: write   # PR comments

jobs:
  validate-yaml:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v5
      - uses: ibiqlik/action-yamllint@v3
        with:
          file_or_dir: ".github/workflows/**/*.yml"
          config_file: .github/.yamllint-config

  filter:
    needs: validate-yaml
    runs-on: ubuntu-latest
    outputs:
      code: ${{ steps.filter.outputs.code }}
      docs: ${{ steps.filter.outputs.docs }}
    steps:
      - uses: actions/checkout@v5
        with:
          fetch-depth: 0
      - name: Filter paths
        id: filter
        uses: dorny/paths-filter@v3
        with:
          filters: |
            code:
              - 'src/**'
              - 'bot/**'
              - 'frontend/**'
              - 'auth/**'
              - 'tests/**'
              - 'scripts/**'
              - 'config/**'
              - '.github/workflows/**'
              - 'pyproject.toml'
              - 'package*.json'
              - '.tool-versions'
              - 'docker-compose*.yaml'
              - '.env*'
              - 'Makefile'
              - 'plugins/**'
            docs:
              - 'docs/**'
              - '**/*.md'
              - '.github/copilot-instructions.md'

      - name: Set up Docker Buildx
        if: steps.guard.outputs.run == 'true'
        uses: docker/setup-buildx-action@v3

      - name: Prepare CI log directory
        if: steps.guard.outputs.run == 'true'
        run: mkdir -p logs

      - name: Create virtual environment
        if: steps.guard.outputs.run == 'true'
        run: |
          python -m venv .venv
          # shellcheck disable=SC1091
          source .venv/bin/activate
          printf 'VIRTUAL_ENV=%s\n' "$VIRTUAL_ENV" >> "$GITHUB_ENV"
          printf '%s/bin\n' "$VIRTUAL_ENV" >> "$GITHUB_PATH"

      - name: Install dev dependencies
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          pip install .[test] 2>&1 | tee logs/pip-install.log || {
            printf 'Pip install failed. See docs/offline-setup.md for offline instructions.\n' >&2
            exit 1
          }
          pip install pip-audit yamllint shellcheck-py

      - name: Lint shell scripts
        if: steps.guard.outputs.run == 'true'
        run: shellcheck --severity=warning scripts/*.sh

      - name: Lint commit messages
        if: steps.guard.outputs.run == 'true'
        run: bash scripts/check_commit_messages.sh

      - name: Check Python dependencies
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          pip check

      - name: Python dependency audit
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          if ! pip-audit; then
            code=$?
            if [ "$code" -eq 1 ]; then
              exit 1
            else
              printf 'pip-audit failed. See docs/offline-setup.md for offline instructions.\n' >&2
            fi
          fi

      - name: Lint and validate bot permissions
        if: steps.guard.outputs.run == 'true'
        run: bash scripts/validate-bot-permissions.sh

      - name: Run Black
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          black --check .

      - name: Regenerate env docs
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/regenerate_env_docs.py

      - name: Validate env docs
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/check_env_docs.py

      - name: Validate Codex Agents
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/validate_agents.py

      - name: Setup environment
        if: steps.guard.outputs.run == 'true'
        run: ./scripts/setup-env.sh

      - name: Install package (editable)
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          pip install -e .[test] 2>&1 | tee logs/pip-install-editable.log

      - name: Enforce Potato ignore policy
        if: steps.guard.outputs.run == 'true'
        run: bash scripts/check_potato_ignore.sh

      - name: Validate PR Summary
        if: steps.guard.outputs.run == 'true' && github.event_name == 'pull_request'
        run: |
          if [ ! -f "PR_SUMMARY.md" ]; then
            printf '::error::PR_SUMMARY.md is required for all pull requests\n'
            printf 'TIP: Use the template: .github/PR_SUMMARY_TEMPLATE.md\n'
            printf 'COPY: cp .github/PR_SUMMARY_TEMPLATE.md PR_SUMMARY.md\n'
            exit 1
          fi
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/validate_pr_summary.py PR_SUMMARY.md

      - name: Quality Control Gate
        if: steps.guard.outputs.run == 'true'
        run: |
          if [[ -x "./scripts/qc_pre_push.sh" ]]; then
            printf "RUNNING: 95%% Quality Control validation...\n"
            ./scripts/qc_pre_push.sh
          else
            printf "WARNING: QC script not found\n"
            printf "Skipping comprehensive validation\n"
          fi

      - name: Generate OpenAPI spec
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/generate_openapi.py

      - name: Check committed OpenAPI spec
        if: steps.guard.outputs.run == 'true'
        run: |
          git diff --quiet src/devonboarder/openapi.json || {
            printf "::error::openapi.json is outdated. Run 'make openapi' and commit the changes.\n"
            exit 1
          }

      - name: Validate OpenAPI contract
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python - <<'PY'
          import json
          from openapi_spec_validator import validate_spec
          with open('src/devonboarder/openapi.json') as f:
              spec = json.load(f)
          try:
              validate_spec(spec)
              print('PASS: OpenAPI spec is valid')
          except Exception as e:
              print(f'FAIL: OpenAPI spec validation failed: {e}')
              raise SystemExit(1)
          PY

      - name: Alembic migration lint
        if: steps.guard.outputs.run == 'true'
        run: ./scripts/alembic_migration_check.sh

      - name: Doc coverage check
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/check_docstrings.py src/devonboarder

      - name: Run ruff
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          ruff check --output-format=github .

      - name: Run mypy
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          mypy --cache-dir=logs/.mypy_cache src/devonboarder

      - name: Generate secrets
        if: steps.guard.outputs.run == 'true'
        run: ./scripts/generate-secrets.sh

      - name: Audit environment variables
        if: steps.guard.outputs.run == 'true'
        run: |
          mkdir -p logs
          env -i PATH="$PATH" bash -c '
            set -a
            source .env.ci
            set +a
            JSON_OUTPUT=logs/env_audit.json bash scripts/audit_env_vars.sh
          ' > logs/env_audit.log
          cat logs/env_audit.log
          cat logs/env_audit.json || true
          missing=$(python -c '
            import json,os
            p="logs/env_audit.json"
            print("".join(json.load(open(p)).get("missing", [])) if os.path.exists(p) else "")
          ')
          extras=$(python -c '
            import json,os
            p="logs/env_audit.json"
            print("".join(
              e for e in (json.load(open(p)).get("extra", []) if os.path.exists(p) else [])
              if e not in ("PATH","PWD","SHLVL","_")
            ))
          ')
          if [ -n "$missing" ] || [ -n "$extras" ]; then
            printf "::error::Environment variable mismatch detected\n"
            exit 1
          fi

      - name: Upload env audit
        if: steps.guard.outputs.run == 'true' && hashFiles('logs/env_audit.json') != ''
        uses: actions/upload-artifact@v4
        with:
          name: env-audit
          path: |
            logs/env_audit.log
            logs/env_audit.json
          retention-days: 7

      - name: Build containers
        if: steps.guard.outputs.run == 'true'
        run: docker compose -f docker-compose.ci.yaml --env-file .env.ci build 2>&1 | tee logs/docker-build.log

      - name: Scan images with Trivy
        if: steps.guard.outputs.run == 'true'
        env:
          TRIVY_VERSION: 0.47.0
        run: bash scripts/trivy_scan.sh docker-compose.ci.yaml

      - name: Start docker compose
        if: steps.guard.outputs.run == 'true'
        run: docker compose -f docker-compose.ci.yaml --env-file .env.ci up -d

      - name: Verify compose services
        if: steps.guard.outputs.run == 'true'
        run: |
          docker compose -f docker-compose.ci.yaml --env-file .env.ci ps
          failed=$(docker compose -f docker-compose.ci.yaml --env-file .env.ci ps -q | xargs -r docker inspect -f '{{.State.Status}}' | grep -v running || true)
          if [ -n "$failed" ]; then
            docker compose -f docker-compose.ci.yaml --env-file .env.ci logs
            exit 1
          fi

      - name: Wait for auth service
        if: steps.guard.outputs.run == 'true'
        run: bash scripts/wait_for_service.sh http://localhost:8002/health 30 2 auth

      - name: Run diagnostics
        if: steps.guard.outputs.run == 'true'
        run: |
          mkdir -p logs
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python -m diagnostics > logs/diagnostics.log

      - name: Upload diagnostics
        if: steps.guard.outputs.run == 'true' && hashFiles('logs/diagnostics.log') != ''
        uses: actions/upload-artifact@v4
        with:
          name: diagnostics
          path: logs/diagnostics.log
          retention-days: 7

      - name: Prepare test-results directory
        if: steps.guard.outputs.run == 'true'
        run: mkdir -p test-results logs

      - name: Run tests with coverage
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          COVERAGE_FILE=logs/.coverage pytest \
            --cache-dir=logs/.pytest_cache \
            --cov=src \
            --cov-report=xml:logs/coverage.xml \
            --cov-fail-under=95 \
            --junitxml=test-results/pytest-results.xml \
            2>&1 | tee logs/pytest.log

      - name: Upload pytest results
        if: steps.guard.outputs.run == 'true' && hashFiles('test-results/pytest-results.xml') != ''
        uses: actions/upload-artifact@v4
        with:
          name: pytest-results
          path: test-results/pytest-results.xml
          retention-days: 7

      - name: Annotate pytest failures
        if: steps.guard.outputs.run == 'true' && failure() && hashFiles('test-results/pytest-results.xml') != ''
        run: |
          line=$(grep -n -m 1 '<failure' test-results/pytest-results.xml | cut -d: -f1)
          if [ -z "$line" ]; then
            line=1
          fi
          printf '::error file=test-results/pytest-results.xml,line=%s::Test failures detected\n' "$line"

      - name: Install frontend dependencies
        if: steps.guard.outputs.run == 'true'
        working-directory: frontend
        run: |
          npm ci || {
            printf 'npm install failed. See docs/offline-setup.md for offline instructions.\n' >&2
            exit 1
          }

      - name: Run frontend lint
        if: steps.guard.outputs.run == 'true'
        working-directory: frontend
        run: npm run lint

      - name: Run frontend tests with coverage
        if: steps.guard.outputs.run == 'true'
        working-directory: frontend
        run: npm test 2>&1 | tee vitest.log

      - name: Upload vitest log
        if: steps.guard.outputs.run == 'true' && hashFiles('frontend/vitest.log') != ''
        uses: actions/upload-artifact@v4
        with:
          name: vitest-log
          path: frontend/vitest.log
          retention-days: 7

      - name: Cache Playwright browsers
        if: steps.guard.outputs.run == 'true'
        uses: actions/cache@v4
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-node${{ matrix.node-version }}-playwright-${{ hashFiles('frontend/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node${{ matrix.node-version }}-playwright-

      - name: Install Playwright browsers
        if: steps.guard.outputs.run == 'true'
        working-directory: frontend
        run: npx playwright install --with-deps

      - name: Run E2E tests
        if: steps.guard.outputs.run == 'true'
        working-directory: frontend
        env:
          AUTH_URL: http://localhost:8002
        run: npm run test:e2e 2>&1 | tee playwright.log

      - name: Run performance tests
        if: steps.guard.outputs.run == 'true'
        working-directory: frontend
        run: npm run perf 2>&1 | tee lhci.log

      - name: Run accessibility tests
        if: steps.guard.outputs.run == 'true'
        working-directory: frontend
        run: npm run test:a11y 2>&1 | tee a11y.log

      - name: Upload playwright log
        if: steps.guard.outputs.run == 'true' && hashFiles('frontend/playwright.log') != ''
        uses: actions/upload-artifact@v4
        with:
          name: playwright-log
          path: frontend/playwright.log
          retention-days: 7

      - name: Upload accessibility log
        if: steps.guard.outputs.run == 'true' && hashFiles('frontend/a11y.log') != ''
        uses: actions/upload-artifact@v4
        with:
          name: a11y-log
          path: frontend/a11y.log
          retention-days: 7

      - name: Upload Lighthouse log
        if: steps.guard.outputs.run == 'true' && hashFiles('frontend/lhci.log') != ''
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-log
          path: frontend/lhci.log
          retention-days: 7

      - name: Upload Lighthouse report
        if: steps.guard.outputs.run == 'true' && hashFiles('frontend/lhci-report/**') != ''
        uses: actions/upload-artifact@v4
        with:
          name: lighthouse-report
          path: frontend/lhci-report
          retention-days: 7

      - name: Install bot dependencies
        if: steps.guard.outputs.run == 'true'
        working-directory: bot
        run: |
          npm ci || {
            printf 'npm install failed. See docs/offline-setup.md for offline instructions.\n' >&2
            exit 1
          }

      - name: Run bot lint
        if: steps.guard.outputs.run == 'true'
        working-directory: bot
        run: npm run lint

      - name: Run bot tests with coverage
        if: steps.guard.outputs.run == 'true'
        working-directory: bot
        run: npm test 2>&1 | tee jest.log

      - name: Upload jest log
        if: steps.guard.outputs.run == 'true' && hashFiles('bot/jest.log') != ''
        uses: actions/upload-artifact@v4
        with:
          name: jest-log
          path: bot/jest.log
          retention-days: 7

      - name: Generate coverage summary
        if: steps.guard.outputs.run == 'true'
        env:
          GITHUB_SERVER_URL: ${{ github.server_url }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_RUN_ID: ${{ github.run_id }}
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/post_coverage_comment.py coverage-summary.md
          bash scripts/append_coverage_summary.sh coverage-summary.md

      - name: Upload coverage summary
        if: steps.guard.outputs.run == 'true' && hashFiles('coverage-summary.md') != ''
        uses: actions/upload-artifact@v4
        with:
          name: coverage-summary
          path: coverage-summary.md
          retention-days: 7

      - name: Upload coverage data
        if: steps.guard.outputs.run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: coverage-data
          path: |
            bot/coverage
            frontend/coverage
            logs/.coverage
            logs/coverage.xml
          retention-days: 7

      - name: Post coverage comment
        if: steps.guard.outputs.run == 'true' && github.event_name == 'pull_request'
        env:
          CI_ISSUE_AUTOMATION_TOKEN: ${{ secrets.CI_ISSUE_AUTOMATION_TOKEN }}
          CI_BOT_TOKEN: ${{ secrets.CI_BOT_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          export GH_TOKEN="${CI_ISSUE_AUTOMATION_TOKEN:-${CI_BOT_TOKEN:-${GITHUB_TOKEN}}}"
          gh pr comment ${{ github.event.pull_request.number }} --body-file coverage-summary.md

      - name: Update coverage badge
        if: steps.guard.outputs.run == 'true'
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/update_coverage_badge.py coverage-summary.md coverage.svg

      - name: Commit coverage badge
        if: steps.guard.outputs.run == 'true'
        env:
          CI_ISSUE_AUTOMATION_TOKEN: ${{ secrets.CI_ISSUE_AUTOMATION_TOKEN }}
          CI_BOT_TOKEN: ${{ secrets.CI_BOT_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # shellcheck disable=SC1091
          source .venv/bin/activate
          git config user.name "${{ github.actor }}"
          git config user.email "${{ github.actor }}@users.noreply.github.com"

          if [ -f "coverage.svg" ]; then
            git add coverage.svg
            if git diff --staged --quiet; then
              printf "No changes to coverage badge\n"
            else
              git commit -m "CHORE(ci): update coverage badge"
              TOKEN="${CI_ISSUE_AUTOMATION_TOKEN:-${CI_BOT_TOKEN:-${GITHUB_TOKEN}}}"
              printf "Pushing badge with selected token\n"
              git push "https://${TOKEN}@github.com/${{ github.repository }}.git" "HEAD:${{ github.ref }}" || {
                printf "FAIL: push failed - uploading badge as artifact\n" >&2
                exit 1
              }
            fi
          else
            printf "Coverage badge file not found\n"
          fi

      - name: Upload coverage badge as artifact (fallback)
        if: steps.guard.outputs.run == 'true' && failure() && hashFiles('coverage.svg') != ''
        uses: actions/upload-artifact@v4
        with:
          name: coverage-badge-fallback
          path: coverage.svg
          retention-days: 30

      - name: Check CORS & security headers
        if: steps.guard.outputs.run == 'true'
        env:
          CHECK_HEADERS_URL: http://localhost:8002/api/user
        run: ./scripts/check_headers.py

      - name: Run security audit
        if: steps.guard.outputs.run == 'true'
        run: bash scripts/security_audit.sh

      - name: Bandit Security Scan
        if: steps.guard.outputs.run == 'true'
        run: bandit -r src -ll

      - name: NPM Audit (frontend, high severity)
        if: steps.guard.outputs.run == 'true'
        working-directory: frontend
        run: npm audit --audit-level=high

      - name: NPM Audit (bot, high severity)
        if: steps.guard.outputs.run == 'true'
        working-directory: bot
        run: npm audit --audit-level=high

      - name: Stop docker compose
        if: always() && steps.guard.outputs.run == 'true'
        run: |
          if [ -f .env.ci ]; then
            docker compose -f docker-compose.ci.yaml --env-file .env.ci down
          else
            docker compose -f docker-compose.ci.yaml down
          fi

      - name: Label Codex PR
        if: steps.guard.outputs.run == 'true' && github.actor == 'codex[bot]'
        env:
          CI_ISSUE_AUTOMATION_TOKEN: ${{ secrets.CI_ISSUE_AUTOMATION_TOKEN }}
          CI_BOT_TOKEN: ${{ secrets.CI_BOT_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          export GH_TOKEN="${CI_ISSUE_AUTOMATION_TOKEN:-${CI_BOT_TOKEN:-${GITHUB_TOKEN}}}"
          if gh api user >/dev/null 2>&1; then
            gh pr edit ${{ github.event.pull_request.number }} --add-label "Codex" || true
          else
            printf '::warning::GitHub CLI authentication failed. Skipping PR labeling.\n'
          fi

      - name: Clean test artifacts
        if: always() && steps.guard.outputs.run == 'true'
        run: bash scripts/clean_pytest_artifacts.sh

      - name: Post-success log cleanup
        if: success() && steps.guard.outputs.run == 'true'
        run: |
          printf 'CI completed successfully - smart cleaning temporary artifacts\n'
          bash scripts/manage_logs.sh smart-clean
          printf 'SUCCESS: Temporary artifacts cleaned, important logs preserved for next run\n'

      - name: Summarize CI failures
        if: always() && steps.guard.outputs.run == 'true'
        run: |
          if [ ! -d ".venv" ]; then
            python -m venv .venv
          fi
          # shellcheck disable=SC1091
          source .venv/bin/activate
          python scripts/summarize_ci_failures.py

      - name: Run CI failure diagnoser
        if: always() && steps.guard.outputs.run == 'true'
        run: |
          if [ ! -d ".venv" ]; then
            python -m venv .venv
          fi
          # shellcheck disable=SC1091
          source .venv/bin/activate

          if python scripts/ci_failure_diagnoser.py ${{ runner.temp }}/_github_workflow/*/job.log > audit.md 2>&1; then
            printf 'CI failure diagnoser completed successfully\n'
          else
            {
              printf '# CI Failure Analysis\n\n'
              printf 'CI failure diagnoser encountered an issue:\n'
              printf '- Log files may not be available at expected location\n'
              printf -- '- Job logs: `%s`\n\n' '${{ runner.temp }}/_github_workflow/*/job.log'
              printf 'Please check the workflow logs for detailed error information.\n'
            } > audit.md
          fi
          if [ ! -s audit.md ]; then
            printf '# CI Status\n' > audit.md
            printf 'No failure patterns detected in available logs.\n' >> audit.md
          fi

      - name: Download previous CI failure issue
        if: always() && steps.guard.outputs.run == 'true'
        env:
          CI_ISSUE_AUTOMATION_TOKEN: ${{ secrets.CI_ISSUE_AUTOMATION_TOKEN }}
          CI_BOT_TOKEN: ${{ secrets.CI_BOT_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          export GH_TOKEN="${CI_ISSUE_AUTOMATION_TOKEN:-${CI_BOT_TOKEN:-${GITHUB_TOKEN}}}"
          bash scripts/download_ci_failure_issue.sh

      - name: Create or update CI failure issue
        if: always() && steps.guard.outputs.run == 'true' && github.event_name == 'pull_request'
        id: ci_failure
        env:
          CI_ISSUE_AUTOMATION_TOKEN: ${{ secrets.CI_ISSUE_AUTOMATION_TOKEN }}
          CI_BOT_TOKEN: ${{ secrets.CI_BOT_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          set -euo pipefail
          printf '\n<!-- sha: %s -->\n' "$GITHUB_SHA" >> summary.md

          if [ -f audit.md ]; then
            cat audit.md >> summary.md
          else
            printf "# CI Analysis\n" >> summary.md
            printf "Audit file not available.\n" >> summary.md
          fi

          export GH_TOKEN="${CI_ISSUE_AUTOMATION_TOKEN:-${CI_BOT_TOKEN:-${GITHUB_TOKEN}}}"
          bash scripts/notify_ci_issue.sh comment "$ISSUE" summary.md || \
            printf "::warning::Failed to notify CI failure for issue %s. Continuing.\n" "$ISSUE"

      - name: Save CI failure issue number
        if: failure() && steps.guard.outputs.run == 'true' && github.event_name == 'pull_request'
        run: printf '%s\n' "${{ steps.ci_failure.outputs.issue-number }}" > ci_failure_issue.txt

      - name: Upload CI failure issue number
        if: failure() && steps.guard.outputs.run == 'true' && github.event_name == 'pull_request' && hashFiles('ci_failure_issue.txt') != ''
        uses: actions/upload-artifact@v4
        with:
          name: ci-failure-issue
          path: ci_failure_issue.txt
          retention-days: 7

      - name: Close CI failure issue
        if: success() && steps.guard.outputs.run == 'true'
        env:
          CI_ISSUE_AUTOMATION_TOKEN: ${{ secrets.CI_ISSUE_AUTOMATION_TOKEN }}
          CI_BOT_TOKEN: ${{ secrets.CI_BOT_TOKEN }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          export GH_TOKEN="${CI_ISSUE_AUTOMATION_TOKEN:-${CI_BOT_TOKEN:-${GITHUB_TOKEN}}}"
          if ! gh api user >/dev/null 2>&1; then
            printf '::warning::GitHub CLI authentication failed. Skipping issue closure.\n'
            exit 0
          fi
          bash scripts/ci_issue_batch_close.sh "ci-failure" "CI run ${{ github.run_id }} passed. Closing."

      - name: Prepare CI log artifacts
        if: always() && steps.guard.outputs.run == 'true'
        run: |
          mkdir -p logs
          mv -f gh_cli.log logs/ 2>/dev/null || true
          mv -f audit.md logs/ 2>/dev/null || true

      - name: Upload CI logs
        if: always() && steps.guard.outputs.run == 'true'
        uses: actions/upload-artifact@v4
        with:
          name: ci-logs
          path: |
            logs
            ${{ runner.temp }}/_github_workflow/*/job.log
